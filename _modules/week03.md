---
title: Week 3, Feb. 6/7
---

### The Meaning of a Text

We extend word2vec's distributional method to sequences of words, allowing us to obtain embeddings that represent 
sentences, paragraphs, and even entire documents. We learn about the BERT model and the Transformer encoder 
architecture that underlies it. In lab, we learn about _natural language inference_ (NLI), a task designed to test a 
model's understanding of natural language sentences.