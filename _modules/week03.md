---
title: Week 3, Feb. 6/7
---

### The Meaning of a Text

We extend word2vec's distributional method to sequences of words, allowing us to obtain embeddings that represent sentences, paragraphs, and even entire documents. We learn about the BERT model and the Transformer encoder architecture that underlies it. We use the Turing test to evaluate BERT's understanding of natural language.

Lecture
: Transformers, transfer learning, foundation models, BERT

Lab
: Natural language inference with BERT

Reading
: **Tutorial**{: .label .label-yellow } [Alammar's (2018a)](https://jalammar.github.io/illustrated-transformer/) tutorial on Transformers, and [Alammar's (2018b)](https://jalammar.github.io/illustrated-bert/) tutorial on BERT
: **SLP**{: .label .label-yellow } [Chapter 10](https://web.stanford.edu/~jurafsky/slp3/10.pdf)
: **D2L**{: .label .label-yellow } [Chapter 11](https://d2l.ai/chapter_attention-mechanisms-and-transformers) (skip Sections 11.2, 11.4, and 11.8), [Sections 15.8—15.10](https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html), and [Sections 16.4—16.7](https://d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html) (skip Section 16.5)
: **LING2**{: .label .label-yellow } [Chapter 11](https://link.springer.com/chapter/10.1007/978-3-031-02172-5_11) and [Chapter 13, #89–90](https://link.springer.com/chapter/10.1007/978-3-031-02172-5_13)
: [Turing (1950)](https://doi.org/10.1093/mind/LIX.236.433), on the imitation game (Turing test)
: The [GLUE](https://gluebenchmark.com/) [(Wang et al., 2019a)](https://openreview.net/pdf?id=rJ4km2R5t7) and [SuperGLUE](https://super.gluebenchmark.com/) [(Wang et al., 2019b)](https://w4ngatang.github.io/static/papers/superglue.pdf) benchmarks