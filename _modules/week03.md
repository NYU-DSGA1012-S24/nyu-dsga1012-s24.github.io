---
title: Week 3, Feb. 6/7
---

### The Meaning of a Text

We extend word2vec's distributional method to sequences of words, allowing us to obtain embeddings that represent 
sentences, paragraphs, and even entire documents. We learn about the BERT model and the Transformer encoder 
architecture that underlies it. In lab, we learn about _natural language inference_ (NLI), a task designed to test a 
model's understanding of natural language sentences.

Lecture
: Stochastic gradient descent, Adam, automatic differentiation, PyTorch

Lab
: Sentiment classification using a multi-layer perceptron

Reading
: **D2L**{: .label .label-yellow } [Section 2.5](https://d2l.ai/chapter_preliminaries/autograd.html) and [Chapter 5](https://d2l.ai/chapter_multilayer-perceptrons)
: [Mikolov et al. (2013)](https://arxiv.org/abs/1301.3781), the word2vec paper
: [Pennington et al. (2014)](https://nlp.stanford.edu/pubs/glove.pdf), the GloVe paper