---
title: Week 2, Jan. 30/31
---

### Deep Learning

We learn how to optimize an arbitrary machine learning objective using the stochastic gradient descent algorithm (SGD) and its more popular variant, Adam. We also learn how automatic differentiation is implemented in the PyTorch software library.

Lecture
: SGD, Adam, automatic differentiation, PyTorch, neural networks, multi-layer perceptrons

Lab
: Sentiment classification using a multi-layer perceptron

Reading
: **Tutorial**{: .label .label-yellow } Week 2 Handout (to be released), [Olah's (2015)](https://colah.github.io/posts/2015-08-Backprop/) tutorial on backpropagation, and [Sanderson's (2017)](https://www.3blue1brown.com/topics/neural-networks) video series on deep learning (optional)
: **SLP**{: .label .label-yellow } [Chapter 7](https://web.stanford.edu/~jurafsky/slp3/7.pdf)
: **D2L**{: .label .label-yellow } [Chapter 1](https://d2l.ai/chapter_introduction), [Section 2.5](https://d2l.ai/chapter_preliminaries/autograd.html), [Chapter 5](https://d2l.ai/chapter_multilayer-perceptrons), [Chapter 12](https://d2l.ai/chapter_optimization/) (skip Sections 12.7â€“12.9), [Section 16.1](https://d2l.ai/chapter_natural-language-processing-applications/index.html), and [Section 19.1](https://d2l.ai/chapter_hyperparameter-optimization) (the rest of the chapter is optional)