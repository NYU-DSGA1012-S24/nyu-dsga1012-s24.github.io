---
title: Week 2, Jan. 30/31
---

### Deep Learning

We learn how to optimize an arbitrary machine learning objective using the stochastic gradient descent algorithm and its 
more popular variant, Adam. We also learn how automatic differentiation is implemented in the PyTorch software library.

Topics
: Stochastic gradient descent, Adam, automatic differentiation, PyTorch

