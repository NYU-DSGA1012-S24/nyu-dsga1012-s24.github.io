---
title: Week 2, Jan. 30/31
---

### Deep Learning

We learn how to optimize an arbitrary machine learning objective using the stochastic gradient descent algorithm (SGD) and its more popular variant, Adam. We also learn how automatic differentiation is implemented in the PyTorch software library.

Lecture
: SGD, Adam, automatic differentiation, PyTorch, neural networks, multi-layer perceptrons
: [Slides](https://drive.google.com/file/d/186uSCNXPQZu8Tfw3CFjCmRem-Nl9mQS8/view?usp=sharing), [Zoom Recording](https://nyu.zoom.us/rec/share/GhAaZOIK06_Bgyvg3vwkkSxmEHd8MzP40FjeiruQNDD08zkBrhCgjQvC61BHKFQ0.caOQ9Y-4pp9mGi7O), [Extra Slides](https://drive.google.com/file/d/1xe2Wja52hLjBMkJ-3DCEsteN7mTYt2eX/view?usp=sharing) (that we didn't get to in class)

Lab
: Sentiment classification using a multi-layer perceptron

Reading
: **Tutorial**{: .label .label-yellow } Week 2 Handout (to be released), [Olah's (2015)](https://colah.github.io/posts/2015-08-Backprop/) tutorial on backpropagation, and [Sanderson's (2017)](https://www.3blue1brown.com/topics/neural-networks) video series on deep learning (optional)
: **SLP**{: .label .label-yellow } [Chapter 7](https://web.stanford.edu/~jurafsky/slp3/7.pdf)
: **D2L**{: .label .label-yellow } [Chapter 1](https://d2l.ai/chapter_introduction), [Section 2.5](https://d2l.ai/chapter_preliminaries/autograd.html), [Chapter 5](https://d2l.ai/chapter_multilayer-perceptrons), [Chapter 12](https://d2l.ai/chapter_optimization/) (skip Sections 12.7â€“12.9), [Section 16.1](https://d2l.ai/chapter_natural-language-processing-applications/index.html), and [Section 19.1](https://d2l.ai/chapter_hyperparameter-optimization) (the rest of the chapter is optional)

Dates
: <span>HW 1 Released Mon 1/29</span>{:.label.label-green} <span>EC 1 Released Mon 1/29</span>{:.label.label-green} <span>RQ 1 Released Fri 2/2</span>{:.label.label-green}