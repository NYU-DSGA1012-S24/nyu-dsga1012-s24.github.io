---
title: Week 2, Jan. 30/31
---

### Deep Learning

We learn how to optimize an arbitrary machine learning objective using the stochastic gradient descent algorithm and its 
more popular variant, Adam. We also learn how automatic differentiation is implemented in the PyTorch software library.


Lecture
: Stochastic gradient descent, Adam, automatic differentiation, PyTorch

Lab
: Sentiment classification using a multi-layer perceptron

Reading
: **D2L**{: .label .label-yellow } [Section 2.5](https://d2l.ai/chapter_preliminaries/autograd.html) and [Chapter 5](https://d2l.ai/chapter_multilayer-perceptrons)
: [Mikolov et al. (2013)](https://arxiv.org/abs/1301.3781), the word2vec paper
: [Pennington et al. (2014)](https://nlp.stanford.edu/pubs/glove.pdf), the GloVe paper