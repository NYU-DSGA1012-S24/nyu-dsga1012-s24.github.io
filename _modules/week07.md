---
title: Week 7, Mar. 5/6
---

### Following Instructions and Being Helpful

We introduce language model alignment, a technique for turning language models into instruction-following assistants like ChatGPT. We learn about reinforcement learning from human feedback (RLHF), the most popular algorithm for alignment. We also explore the intellectual origins of alignment in the Effective Altruism movement.

Lecture
: Alignment, RLHF, DPO, InstructGPT, AI safety

Lab
: TBD

Reading
: **Notes**{: .label .label-yellow } Week 6 Handout: Reinforcement Learning and Alignment
: [Miles (2014)](https://www.huffpost.com/entry/artificial-intelligence-oxford_n_5689858), an interview with Nick Bostrom on the paperclip problem
: [Levitz (2022)](https://nymag.com/intelligencer/2022/08/why-effective-altruists-fear-the-ai-apocalypse.html), an interview with Will MacAskill on effective altruism and alignment
: [Askell et al. (2021)](https://arxiv.org/abs/2112.00861), on alignment and the HHH criteria
: [The InstructGPT paper (Ouyang et al., 2022)](https://arxiv.org/abs/2203.02155), on RLHF
: [Rafailov et al. (2023)](https://arxiv.org/abs/2305.18290), on DPO
