---
title: Week 7, Mar. 5/6
---

### Following Instructions and Being Helpful

We introduce language model alignment, a technique for turning language models into instruction-following assistants like ChatGPT. We learn about reinforcement learning from human feedback (RLHF), the most popular algorithm for alignment. We also explore the intellectual origins of alignment in the Effective Altruism movement.

Lecture
: Alignment, RLHF, DPO, InstructGPT, AI safety
: [Slides](https://drive.google.com/file/d/1dat9TNLvu61WJ4DLFleV2O3wAkJ_6CDu/view?usp=sharing), [Zoom Recording](https://nyu.zoom.us/rec/share/JAjEJlUtfvja2BsezTwkj_13vw-dujgTN6GbKQ9TPYpnqaSisMyGJOcqtEmXR-bu.Y-sbhiUIR3n1qjeD), [Extra Slides](https://drive.google.com/file/d/1FpF0t6udBXMCPP53yeRNIxJnXGboLF-v/view?usp=drive_link) (that we didn't get to in class)

Lab
: Mini-proposal requirements and peer review
: [Slides](https://drive.google.com/file/d/1X6crhWDiSPBLNJ1wKqGFv4_lSsOwWSwA/view?usp=sharing)

Reading
: **Tutorial**{: .label .label-yellow } Week 7 Handout (to be released)
: [Miles (2014)](https://www.huffpost.com/entry/artificial-intelligence-oxford_n_5689858), [Lewis-Kraus (2022)](https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism), and [Levitz (2022)](https://nymag.com/intelligencer/2022/08/why-effective-altruists-fear-the-ai-apocalypse.html), on Effective Altruism and alignment
: [Bommasani et al. (2021)](https://arxiv.org/abs/2108.07258), motivating alignment
: [Askell et al. (2021)](https://arxiv.org/abs/2112.00861), on alignment and the HHH criteria
: [The InstructGPT paper (Ouyang et al., 2022)](https://arxiv.org/abs/2203.02155) on RLHF and [Rafailov et al. (2023)](https://arxiv.org/abs/2305.18290) on direct preference optimization (DPO)

Dates
: <span>HW 3 Due</span>{:.label.label-red} 
