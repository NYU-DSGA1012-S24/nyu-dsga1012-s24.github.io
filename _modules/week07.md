---
title: Week 7, Mar. 5/6
---

### Following Instructions and Being Helpful

We introduce language model alignment, a technique for turning language models into instruction-following assistants like ChatGPT. We learn about reinforcement learning from human feedback (RLHF), the most popular algorithm for alignment. We also explore the intellectual origins of alignment in the Effective Altruism movement.

Lecture
: Alignment, RLHF, DPO, InstructGPT, AI safety

Lab
: TBD

Reading
: **Tutorial**{: .label .label-yellow } Week 7 Handout (to be released)
: [Miles (2014)](https://www.huffpost.com/entry/artificial-intelligence-oxford_n_5689858), [Lewis-Kraus (2022)](https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism), and [Levitz (2022)](https://nymag.com/intelligencer/2022/08/why-effective-altruists-fear-the-ai-apocalypse.html), on Effective Altruism and alignment
: [Bommasani et al. (2021)](https://arxiv.org/abs/2108.07258), motivating alignment
: [Askell et al. (2021)](https://arxiv.org/abs/2112.00861), on alignment and the HHH criteria
: [The InstructGPT paper (Ouyang et al., 2022)](https://arxiv.org/abs/2203.02155) on RLHF and [Rafailov et al. (2023)](https://arxiv.org/abs/2305.18290) on direct preference optimization (DPO)
