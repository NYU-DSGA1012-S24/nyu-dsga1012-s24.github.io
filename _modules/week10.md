---
title: Week 10, Apr. 2/3
---

### Logic and Reasoning

Can language models engage in complex, multi-step logical reasoning? We survey various Turing tests for reasoning skills, as well as ways to help language models reason better. Guest lecturer Will Merrill, an NYU PhD student in Data Science, will tell us about theoretical limitations imposed by the Transformer architecture on language model reasoning abilities.

Lecture
: Interpretability and model analysis, targeted challenge benchmarks, attention visualization, probing

Lab
: TBD

Reading
: [Kojima et al. (2022)](https://arxiv.org/abs/2205.11916), [Saparov and He (2023)](https://arxiv.org/abs/2210.01240), [Saparov et al. (2023)](https://arxiv.org/abs/2305.15269), and [Press et al. (2023)](https://arxiv.org/abs/2210.03350), on multi-hop reasoning
: [Li et al. (2018)](https://arxiv.org/abs/2106.00737) and [Kim and Schuster (2023)](https://arxiv.org/abs/2305.02363), on world models and entity state tracking
: [Weiss et al. (2021)](https://arxiv.org/abs/2106.06981), [Merrill et al. (2022)](https://arxiv.org/abs/2106.16213), and [Merrill and Sabharwal (2023)](https://arxiv.org/abs/2310.07923), on the computational expressive power of Transformers
: [Yang et al. (2023)](https://arxiv.org/abs/2306.15626) and [Trinh et al. (2024)](https://www.nature.com/articles/s41586-023-06747-5): enhancing the reasoning capabilities of language models

Dates
: <span>Project Full Proposal Due</span>{:.label.label-red}