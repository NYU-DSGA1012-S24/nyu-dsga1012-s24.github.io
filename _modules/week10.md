---
title: Week 10, Apr. 2/3
---

### Logic, Reasoning, and Compositionality

Can language models engage in complex, multi-step logical reasoning? We survey various Turing tests for reasoning skills, as well as ways to help language models reason better. Guest lecturer Will Merrill, an NYU PhD student in Data Science, will tell us about theoretical limitations imposed by the Transformer architecture on language model reasoning abilities.

Lecture
: Interpretability and model analysis, targeted challenge benchmarks, attention visualization, probing

Lab
: TBD

Reading
: [Lake and Baroni (2018)](https://arxiv.org/abs/1711.00350), [Hupkes et al. (2020)](https://arxiv.org/abs/1908.08351), and [Kim and Linzen (2020)](https://arxiv.org/abs/2010.05465), on compositionality
: [Kim and Schuster (2023)](https://arxiv.org/abs/2305.02363), on entity state tracking
: []