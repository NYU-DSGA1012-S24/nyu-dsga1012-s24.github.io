---
title: Week 4, Feb. 13/14
---

### Passing the Turing Test?

BERT's ability to identify relationships between sentences is impressive. How does BERT do it? We examine what strategies BERT uses to beat the NLI Turing test, and we analyze what information is represented in BERT's hidden representations.

Lecture
: Interpretability and model analysis, targeted challenge benchmarks, attention visualization, probing

Lab
: TBD

Reading
: [Gurungan et al. (2018)](https://aclanthology.org/N18-2017/) and [McCoy et al. (2019)](https://arxiv.org/abs/1902.01007), on gaming the system for NLI
: [Kocijan et al. (2022)](https://arxiv.org/abs/2201.02387), _The Defeat of the Winograd Schema Challenge_
: [Rogers et al. (2020)](https://arxiv.org/abs/2002.12327), a survey on what we know about BERT
: [Hao and Linzen (2023)](https://arxiv.org/abs/2310.15151), on probing and causality 
: [Jain and Wallace (2019)](https://arxiv.org/abs/1902.10186), [Wiegreffe and Pinter (2019)](https://arxiv.org/abs/1908.04626), and [Belinkov (2022)](https://aclanthology.org/2022.cl-1.7/): critical takes on interpretability techniques and findings