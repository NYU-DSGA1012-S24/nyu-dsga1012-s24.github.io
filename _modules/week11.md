---
title: Week 11, Apr. 9/10
---

### Artificial vs. Human Intelligence

Humans acquire, generate, and process language in a very particular way. We explore ways in which humans and language models are remarkably similar in their language processing capabilities, yet also incredibly different. We review recent attempts to train language models in a way that mimics the developmental process of human language acquisition.

Lecture
: Surprisal theory, generalization, inductive bias, poverty of the stimulus, language acquisition

Lab
: TBD

Reading
: [Wilcox et al. (2023)](https://doi.org/10.1162/tacl_a_00612), [Oh and Schuler (2023)](https://doi.org/10.1162/tacl_a_00548), and [Huang et al. (2023)](https://osf.io/preprints/psyarxiv/z38u6), on surprisal theory
: [Hupkes et al. (2020)](https://arxiv.org/abs/1908.08351), [Kim and Linzen (2020)](https://arxiv.org/abs/2010.05465), and [Kim et al. (2022)](https://arxiv.org/abs/2212.10769), on compositionality
: [Mueller et al. (2022)](https://arxiv.org/abs/2203.09397) and [Mueller et al. (2023)](https://arxiv.org/abs/2305.19905), on hierarchical generalization
: [Linzen (2020)](https://arxiv.org/abs/2005.00955), [Yedetore et al. (2023)](https://arxiv.org/abs/2301.11462) and [Warstadt et al. (2023)](https://aclanthology.org/volumes/2023.conll-babylm/), on pre-training vs. human language acquisition

Dates
: <span>RQ 5 Released Fri 4/12</span>{:.label.label-green} <span>Project Full Proposal Due</span>{:.label.label-red}